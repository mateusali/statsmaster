{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dfbe382-61db-4eb8-927f-964ac228e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cctest(df,num_var,cat_var=None,num_var2=None, cc=False, independent=True):\n",
    "    \n",
    "    \"\"\" \n",
    "    Hypothesis testing\n",
    "    \n",
    "    For Normality tests: Null hypotheses is the data is normaly distributed\n",
    "\n",
    "    For equality of variance: Null hypotheses is all groups have equal varinces\n",
    "\n",
    "    For test of independency: Null hypotheses is \"there is no association/correlation between thoes variables\"\n",
    "\n",
    "    If p-value is less than 5%( 0.05) , we reject the null hypotheses at 5% significant level\n",
    "    \n",
    "    from scipy.stats import mannwhitneyu\n",
    "    import scipy.stats as stats\n",
    "    from scipy.stats import f_oneway\n",
    "    import pingouin as pg\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    from scipy.stats import kruskal\n",
    "    from scipy.stats import wilcoxon\n",
    "    \n",
    "    print(\"Find type test and aplly it for our Continous(num_var) and Categorical(cat_var) variables\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    #Cheking how many obs in dataframe to decide what test we should use to check normallity \n",
    "    num_observations = df.shape[0]\n",
    "    select_test = 'shapiro' if num_observations <= 5000 else 'normaltest'\n",
    "\n",
    "    #Set data by passed variables\n",
    "\n",
    "    group_count = df[cat_var].unique()\n",
    "    i=0\n",
    "    all_var = []\n",
    "    group_name = []\n",
    "    for col in group_count:\n",
    "        #print(col)\n",
    "        var_i = df[num_var][df[cat_var]==col]\n",
    "        all_var.append(var_i)\n",
    "        group_name.append(col)\n",
    "        i+=1\n",
    "        \n",
    "    data1 = all_var[0]\n",
    "    data2 = all_var[1]\n",
    "    data_pvalue1 = getattr(stats,select_test)(data1)[1]\n",
    "    data_pvalue2 = getattr(stats,select_test)(data2)[1]\n",
    "        \n",
    "    if cc is False:\n",
    "        for col in group_count:\n",
    "        #print(col)\n",
    "            var_i_2 = df[num_var2][df[cat_var]==col]\n",
    "            all_var2.append(var_i_2)\n",
    "            group_name.append(col)\n",
    "            i+=1\n",
    "        \n",
    "        data3 = all_var2[0]\n",
    "        data4 = all_var2[0]\n",
    "    \n",
    "    #definition_group = int(input(\"| Type 1 if the Groups are INDEPENDENT - only one observation | \\n| Type 0 if the groups are DEPENDENT - multiple observations  |\\n\"))\n",
    "    if independent is True:\n",
    "        print(\"\\033[1mGroups are INDEPENDENT\\033[0m\")\n",
    "        #print(\"Checking if data is normally distributed\")\n",
    "\n",
    "        # print(data1)\n",
    "        # print(\"-\"*100)\n",
    "         \n",
    "\n",
    "\n",
    "        #Check if data is normally distributed \n",
    "        if (data_pvalue1 > 0.05)  & (data_pvalue2 > 0.05):\n",
    "            print(f\"\\033[1mGroups is normally distributed:\\033[0m Since your p-value for variables {group_name[0]}({round(data_pvalue1,3)}) and {group_name[1]}({round(data_pvalue2,3)}) variables is greater than 0.05,\\n\\033[1mWe fail to reject Null hypotheses.\\033[0m\")    \n",
    "            print(\"-\"*100)\n",
    "            if ((num_var is not None) & (cat_var is not None)) & (df is not None):\n",
    "\n",
    "                #cat_var = input(\"Informe the name of your categorical variable\") \n",
    "                #Checking how many groups\n",
    "                if len(df[cat_var].unique()) == 2:\n",
    "                    print(f\"You are compare {len(df[cat_var].unique())} groups\")\n",
    "                    \n",
    "                    # change to check equals variance (Checking the Homogeneity of Variances Assumption) use levene test or stats.bartlett\n",
    "                    pvalue_groups = stats.levene(data1,data2)[0]\n",
    "                    \n",
    "                    if (pvalue_groups > 0.05):\n",
    "                        print(f\"Since your p-value {pvalue_groups} for equals variance is greater than 0.05, \\nwe considere EQUAL group variances\")\n",
    "                        print(\"To check if exist some correlation: Use Pooled t-test - Parametric test\")\n",
    "                        result = stats.ttest_ind(data1, data2, equal_var=True)\n",
    "                        print(result)\n",
    "\n",
    "                    else:\n",
    "                        print(f\"Since your p-value {pvalue_groups} for equals variance is less than 0.05, \\nwe considere UNEQUAL group variances\")\n",
    "                        print(\"To check if exist some correlation: We used Satterthwaite (also known as Welch's) t-test\")\n",
    "                        result = stats.ttest_ind(data1, data2, equal_var=False)\n",
    "                        print(result)\n",
    "\n",
    "                else:\n",
    "                    print(f\"You are compare {len(df[cat_var].unique())} groups\")\n",
    "\n",
    "                    # group_count = df[cat_var].unique()\n",
    "                    # i=1\n",
    "                    # all_var = []\n",
    "                    # for col in group_count:\n",
    "                    #     var_i = df[num_var][df[cat_var]==col]\n",
    "                    #     all_var.append(var_i)\n",
    "                    #     i+=1\n",
    "                        \n",
    "                    pvalue_groups = stats.levene(*all_var)\n",
    "                    #print(pvalue_groups)\n",
    "                    if (pvalue_groups[1] > 0.05):\n",
    "                        print(f\"Since your p-value {pvalue_groups} for equals variance is greater than 0.05, \\nwe considere EQUAL group variances\")\n",
    "                        print(\"To check if exist some correlation: Use One-way ANOVA - Parametric test\")\n",
    "                        oneway_anova_result = f_oneway(*all_var)\n",
    "                        print(oneway_anova_result)\n",
    "                    else:\n",
    "                        print(f\"Since your p-value {pvalue_groups} for equals variance is less than 0.05, \\nwe considere UNEQUAL group variances\")\n",
    "                        print(\"To check if exist some correlation: Welch's ANOVA\")\n",
    "                        anova_results = pg.welch_anova(dv=num_var, between=cat_var, data=df)\n",
    "                        print(anova_results)\n",
    "\n",
    "            else:\n",
    "                print(\"You should informe numeric and categoric variable and dataframe name\")       \n",
    "        else:\n",
    "            print(\"Before we apply some test especific for data that dosen't follow NORMALLY DISTRIBUITION we need try converting it, using SQRT, LOG or\\\n",
    "            BOXCOX, if we still didn't get a NORMALLY DISTRIBUITION, we can continous here, follow the checks and aplly the recommended test.\")            \n",
    "           \n",
    "            ''' 1.Rely on the Central Limit Theorem if the sample size is large enough (n>30)\n",
    "                2.Use a non-parametric statistical test\n",
    "                3.Transform the data'''\n",
    "            \n",
    "            #print(f\"Since your p-value for boths {data1.name}({data_pvalue1}) and {data2.name}({data_pvalue2}) variables is less than 0.05,\\nwe can considere data \\033[1mIS NOT NORMALLY DISTRIBUTED\\033[0m\")\n",
    "            print(f\"\\033[1mGroups is NOT normally distributed:\\033[0m Since your p-value for variables {data1.name}({round(data_pvalue1,3)}) and {data2.name}({round(data_pvalue2,3)}) variables is less than 0.05,\\n\\033[1mWe reject Null hypotheses.\\033[0m\")  \n",
    "            #cat_var = input(\"Informe the name of your categorical variable\")\n",
    "            if len(df[cat_var].unique()) <= 2:\n",
    "                print(f\"You are compare {len(df[cat_var].unique())} groups\")\n",
    "                \n",
    "                # i=1\n",
    "                # all_var = []\n",
    "                # for col in group_count:\n",
    "                #     var_i = df[num_var][df[cat_var]==col]\n",
    "                #     all_var.append(var_i)\n",
    "                #     i+=1\n",
    "\n",
    "                print(\"To check if exist some correlation: we used test: Mann Whitney U\")\n",
    "                result = mannwhitneyu(*all_var)\n",
    "                print(result)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                print(f\"You are compare {len(df[cat_var].unique())} groups\")\n",
    "\n",
    "                # group_count = df[cat_var].unique()\n",
    "                # i=1\n",
    "                # all_var = []\n",
    "                # for col in group_count:\n",
    "                #     var_i = df[num_var][df[cat_var]==col]\n",
    "                #     all_var.append(var_i)\n",
    "                #     i+=1\n",
    "                print(stats.levene(*all_var))\n",
    "                print(\"To check if exist some correlation: Use Kruskal-Wallis test\") \n",
    "                kruskal_result = kruskal(*all_var)\n",
    "                print(kruskal_result)\n",
    "    #ELSE TO START ANALYSIS IN DEPENDENT GROUPS         \n",
    "    else:\n",
    "        # print(\"Groups are dependent - multiple observations per person. Let's check sampling_difference\")\n",
    "        # var1 = input(\"Inform the first variable to calculate variance (variance square of std): \")\n",
    "        # print(\"-\"*100)\n",
    "        # var2 = input(\"Inform the second variable to calculate variance (variance square of std): \")\n",
    "        # print(\"-\"*100)\n",
    "        # group_count = df[cat_var].unique()\n",
    "        #             i=1\n",
    "        #             all_var = []\n",
    "        #             for col in group_count:\n",
    "        #                 var_i = df[num_var][df[cat_var]==col]\n",
    "        #                 all_var.append(var_i)\n",
    "        #                 i+=1\n",
    "\n",
    "        if cc is False:\n",
    "            sampling_difference = data2 - data1\n",
    "            differences_pvalue = getattr(stats,select_test)(sampling_difference)[1]\n",
    "        \n",
    "        #Cheking if Group differences are normally distributed\n",
    "        if differences_pvalue > 0.05:\n",
    "            print(f\"Since your p-value {round(differences_pvalue,3)} is greater than 0.05 between the two groups, we fail to reject null hypothese and \\nwe conclude that group differences is normaly distributed.\")\n",
    "            #cat_var = input(\"Informe the name of your categorical variable\")\n",
    "                #Checking how many groups\n",
    "            if len(df[cat_var].unique()) <= 2:\n",
    "                print(\"You are compare 2 groups\")\n",
    "                print(\"To check if exist some correlation: Use Paired t-test\")\n",
    "                ttest_rel(before, after)\n",
    "\n",
    "            else:\n",
    "                print(\"You are compare 3 groups or more\")\n",
    "                print(\"To check if exist some correlation: Use Single factor repeated measures ANOVA\")\n",
    "                \n",
    "                model = ols('value ~ C(factor) + C(subject)', data=df).fit()\n",
    "                anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "                      \n",
    "        else:\n",
    "            print(f\"Since your p-value {round(differences_pvalue,3)} is less than 0.05 between the two groups, we fail to reject null hypothese and \\nwe conclude that group differences are NOT normaly distributed.\")\n",
    "            if len(df[cat_var].unique()) <= 2:\n",
    "                print(\"You are compare 2 groups\")\n",
    "                print(\"To check if exist some correlation: Use Wilcoxon Signed Rank\")\n",
    "                wilcoxon_result = wilcoxon(data1, data2)\n",
    "                print(wilcoxon_result)\n",
    "            else:\n",
    "                print(\"You are compare 3 groups or more\")\n",
    "                print(\"To check if exist some correlation: Use Friedman test\")\n",
    "    \n",
    "    return\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    #cc = columns comparation, if is false we dont need set num_var2\n",
    "    from scipy.stats import mannwhitneyu\n",
    "    import scipy.stats as stats\n",
    "    from scipy.stats import f_oneway\n",
    "    import pingouin as pg\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.formula.api import ols\n",
    "    from scipy.stats import kruskal\n",
    "    from scipy.stats import wilcoxon\n",
    "    \n",
    "    print(\"Find type test and aplly it for our Continous(num_var) and Categorical(cat_var) variables\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    #Cheking how many obs in dataframe to decide what test we should use to check normallity \n",
    "    num_observations = df.shape[0]\n",
    "    select_test = 'shapiro' if num_observations <= 5000 else 'normaltest'\n",
    "\n",
    "    #Set data by passed variables\n",
    "\n",
    "    group_count = df[cat_var].unique()\n",
    "    i=0\n",
    "    all_var = []\n",
    "    group_name = []\n",
    "    for col in group_count:\n",
    "        #print(col)\n",
    "        var_i = df[num_var][df[cat_var]==col]\n",
    "        all_var.append(var_i)\n",
    "        group_name.append(col)\n",
    "        i+=1\n",
    "        \n",
    "    data1 = all_var[0]\n",
    "    data2 = all_var[1]\n",
    "    data_pvalue1 = getattr(stats,select_test)(data1)[1]\n",
    "    data_pvalue2 = getattr(stats,select_test)(data2)[1]\n",
    "        \n",
    "    if cc is False:\n",
    "        for col in group_count:\n",
    "        #print(col)\n",
    "            var_i_2 = df[num_var2][df[cat_var]==col]\n",
    "            all_var2.append(var_i_2)\n",
    "            group_name.append(col)\n",
    "            i+=1\n",
    "        \n",
    "        data3 = all_var2[0]\n",
    "        data4 = all_var2[0]\n",
    "    \n",
    "    #definition_group = int(input(\"| Type 1 if the Groups are INDEPENDENT - only one observation | \\n| Type 0 if the groups are DEPENDENT - multiple observations  |\\n\"))\n",
    "    if independent is True:\n",
    "        print(\"\\033[1mGroups are INDEPENDENT\\033[0m\")\n",
    "        #print(\"Checking if data is normally distributed\")\n",
    "\n",
    "        # print(data1)\n",
    "        # print(\"-\"*100)\n",
    "         \n",
    "\n",
    "\n",
    "        #Check if data is normally distributed \n",
    "        if (data_pvalue1 > 0.05)  & (data_pvalue2 > 0.05):\n",
    "            print(f\"\\033[1mGroups is normally distributed:\\033[0m Since your p-value for variables {group_name[0]}({round(data_pvalue1,3)}) and {group_name[1]}({round(data_pvalue2,3)}) variables is greater than 0.05,\\n\\033[1mWe fail to reject Null hypotheses.\\033[0m\")    \n",
    "            print(\"-\"*100)\n",
    "            if ((num_var is not None) & (cat_var is not None)) & (df is not None):\n",
    "\n",
    "                #cat_var = input(\"Informe the name of your categorical variable\") \n",
    "                #Checking how many groups\n",
    "                if len(df[cat_var].unique()) == 2:\n",
    "                    print(f\"You are compare {len(df[cat_var].unique())} groups\")\n",
    "                    \n",
    "                    # change to check equals variance (Checking the Homogeneity of Variances Assumption) use levene test or stats.bartlett\n",
    "                    pvalue_groups = stats.levene(data1,data2)[0]\n",
    "                    \n",
    "                    if (pvalue_groups > 0.05):\n",
    "                        print(f\"Since your p-value {pvalue_groups} for equals variance is greater than 0.05, \\nwe considere EQUAL group variances\")\n",
    "                        print(\"To check if exist some correlation: Use Pooled t-test - Parametric test\")\n",
    "                        result = stats.ttest_ind(data1, data2, equal_var=True)\n",
    "                        print(result)\n",
    "\n",
    "                    else:\n",
    "                        print(f\"Since your p-value {pvalue_groups} for equals variance is less than 0.05, \\nwe considere UNEQUAL group variances\")\n",
    "                        print(\"To check if exist some correlation: We used Satterthwaite (also known as Welch's) t-test\")\n",
    "                        result = stats.ttest_ind(data1, data2, equal_var=False)\n",
    "                        print(result)\n",
    "\n",
    "                else:\n",
    "                    print(f\"You are compare {len(df[cat_var].unique())} groups\")\n",
    "\n",
    "                    # group_count = df[cat_var].unique()\n",
    "                    # i=1\n",
    "                    # all_var = []\n",
    "                    # for col in group_count:\n",
    "                    #     var_i = df[num_var][df[cat_var]==col]\n",
    "                    #     all_var.append(var_i)\n",
    "                    #     i+=1\n",
    "                        \n",
    "                    pvalue_groups = stats.levene(*all_var)\n",
    "                    #print(pvalue_groups)\n",
    "                    if (pvalue_groups[1] > 0.05):\n",
    "                        print(f\"Since your p-value {pvalue_groups} for equals variance is greater than 0.05, \\nwe considere EQUAL group variances\")\n",
    "                        print(\"To check if exist some correlation: Use One-way ANOVA - Parametric test\")\n",
    "                        oneway_anova_result = f_oneway(*all_var)\n",
    "                        print(oneway_anova_result)\n",
    "                    else:\n",
    "                        print(f\"Since your p-value {pvalue_groups} for equals variance is less than 0.05, \\nwe considere UNEQUAL group variances\")\n",
    "                        print(\"To check if exist some correlation: Welch's ANOVA\")\n",
    "                        anova_results = pg.welch_anova(dv=num_var, between=cat_var, data=df)\n",
    "                        print(anova_results)\n",
    "\n",
    "            else:\n",
    "                print(\"You should informe numeric and categoric variable and dataframe name\")       \n",
    "        else:\n",
    "            print(\"Before we apply some test especific for data that dosen't follow NORMALLY DISTRIBUITION we need try converting it, using SQRT, LOG or\\\n",
    "            BOXCOX, if we still didn't get a NORMALLY DISTRIBUITION, we can continous here, follow the checks and aplly the recommended test.\")            \n",
    "           \n",
    "            ''' 1.Rely on the Central Limit Theorem if the sample size is large enough (n>30)\n",
    "                2.Use a non-parametric statistical test\n",
    "                3.Transform the data'''\n",
    "            \n",
    "            #print(f\"Since your p-value for boths {data1.name}({data_pvalue1}) and {data2.name}({data_pvalue2}) variables is less than 0.05,\\nwe can considere data \\033[1mIS NOT NORMALLY DISTRIBUTED\\033[0m\")\n",
    "            print(f\"\\033[1mGroups is NOT normally distributed:\\033[0m Since your p-value for variables {data1.name}({round(data_pvalue1,3)}) and {data2.name}({round(data_pvalue2,3)}) variables is less than 0.05,\\n\\033[1mWe reject Null hypotheses.\\033[0m\")  \n",
    "            #cat_var = input(\"Informe the name of your categorical variable\")\n",
    "            if len(df[cat_var].unique()) <= 2:\n",
    "                print(f\"You are compare {len(df[cat_var].unique())} groups\")\n",
    "                \n",
    "                # i=1\n",
    "                # all_var = []\n",
    "                # for col in group_count:\n",
    "                #     var_i = df[num_var][df[cat_var]==col]\n",
    "                #     all_var.append(var_i)\n",
    "                #     i+=1\n",
    "\n",
    "                print(\"To check if exist some correlation: we used test: Mann Whitney U\")\n",
    "                result = mannwhitneyu(*all_var)\n",
    "                print(result)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                print(f\"You are compare {len(df[cat_var].unique())} groups\")\n",
    "\n",
    "                # group_count = df[cat_var].unique()\n",
    "                # i=1\n",
    "                # all_var = []\n",
    "                # for col in group_count:\n",
    "                #     var_i = df[num_var][df[cat_var]==col]\n",
    "                #     all_var.append(var_i)\n",
    "                #     i+=1\n",
    "                print(stats.levene(*all_var))\n",
    "                print(\"To check if exist some correlation: Use Kruskal-Wallis test\") \n",
    "                kruskal_result = kruskal(*all_var)\n",
    "                print(kruskal_result)\n",
    "    #ELSE TO START ANALYSIS IN DEPENDENT GROUPS         \n",
    "    else:\n",
    "        # print(\"Groups are dependent - multiple observations per person. Let's check sampling_difference\")\n",
    "        # var1 = input(\"Inform the first variable to calculate variance (variance square of std): \")\n",
    "        # print(\"-\"*100)\n",
    "        # var2 = input(\"Inform the second variable to calculate variance (variance square of std): \")\n",
    "        # print(\"-\"*100)\n",
    "        # group_count = df[cat_var].unique()\n",
    "        #             i=1\n",
    "        #             all_var = []\n",
    "        #             for col in group_count:\n",
    "        #                 var_i = df[num_var][df[cat_var]==col]\n",
    "        #                 all_var.append(var_i)\n",
    "        #                 i+=1\n",
    "\n",
    "        if cc is False:\n",
    "            sampling_difference = data2 - data1\n",
    "            differences_pvalue = getattr(stats,select_test)(sampling_difference)[1]\n",
    "        \n",
    "        #Cheking if Group differences are normally distributed\n",
    "        if differences_pvalue > 0.05:\n",
    "            print(f\"Since your p-value {round(differences_pvalue,3)} is greater than 0.05 between the two groups, we fail to reject null hypothese and \\nwe conclude that group differences is normaly distributed.\")\n",
    "            #cat_var = input(\"Informe the name of your categorical variable\")\n",
    "                #Checking how many groups\n",
    "            if len(df[cat_var].unique()) <= 2:\n",
    "                print(\"You are compare 2 groups\")\n",
    "                print(\"To check if exist some correlation: Use Paired t-test\")\n",
    "                ttest_rel(before, after)\n",
    "\n",
    "            else:\n",
    "                print(\"You are compare 3 groups or more\")\n",
    "                print(\"To check if exist some correlation: Use Single factor repeated measures ANOVA\")\n",
    "                \n",
    "                model = ols('value ~ C(factor) + C(subject)', data=df).fit()\n",
    "                anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "                      \n",
    "        else:\n",
    "            print(f\"Since your p-value {round(differences_pvalue,3)} is less than 0.05 between the two groups, we fail to reject null hypothese and \\nwe conclude that group differences are NOT normaly distributed.\")\n",
    "            if len(df[cat_var].unique()) <= 2:\n",
    "                print(\"You are compare 2 groups\")\n",
    "                print(\"To check if exist some correlation: Use Wilcoxon Signed Rank\")\n",
    "                wilcoxon_result = wilcoxon(data1, data2)\n",
    "                print(wilcoxon_result)\n",
    "            else:\n",
    "                print(\"You are compare 3 groups or more\")\n",
    "                print(\"To check if exist some correlation: Use Friedman test\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807213b0-13ef-4b8e-adc6-d9804bcee7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day5():\n",
    "\n",
    "    \"\"\"# Day 5: Homework Exercises\n",
    "    Topics Covered\n",
    "    1. Collinearity\n",
    "    2. Multicollinearity\n",
    "    3. Building a Multiple Linear Regression Model (Detect overfitting)\n",
    "    4. Feature Selection\n",
    "    5. Cross-Validation (calculate multiple regression metrics)\n",
    "    \n",
    "    Use the ToyotaCorolla.csv dataset for the homework exercises. Read the csv file into a dataframe called car_df.\n",
    "    \n",
    "    \n",
    "    # Topic 1: Collinearity\n",
    "    \n",
    "    # data manipulation libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pylab as pl\n",
    "    \n",
    "    # plotting libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    #Stats liberies\n",
    "    from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import KFold\n",
    "    import statsmodels.api as sm\n",
    "    import scipy.stats as stats\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    \n",
    "    #Import dataset\n",
    "    cars_df = pd.read_csv(\"ToyotaCorolla.csv\")\n",
    "    \n",
    "    cars_df.info()\n",
    "    \n",
    "    pd.set_option('display.max_columns', 50)\n",
    "    cars_df.head()\n",
    "    \n",
    "    cars_df.describe()\n",
    "    \n",
    "    cars_df.groupby('Fuel_Type')[\"Price\"].mean()\n",
    "    cars_df['Cylinders'].value_counts()\n",
    "    \n",
    "    - Q1_1 (Basic): What is collinearity. Find the correlation between the numeric variables in cars_df.\n",
    "    ##### R: Collinearity is a strong/high correlation between two independent variables (features) and they are used as predictors for the target variable(DAY5 Recording: 02:30:16)\n",
    "    \n",
    "    cars_df.corr(numeric_only=True)\n",
    "    #We can see Cylinders varible is Null because no correlation between then and others variables since the values is the same for all obs the std is 0 \n",
    "    #then in pearson formula covariance between x and y was divide by std. \n",
    "    \n",
    "    ## Q1_2 (Moderate): Create a heatmap that shows the correlation between the numeric variables in cars_df. Which variables exhibit high collinearity ?\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    sns.heatmap(cars_df.corr(numeric_only=True),cmap='coolwarm')\n",
    "    \n",
    "    \n",
    "    X = cars_df.drop(columns=['Id','Price'], axis=1)\n",
    "    y = cars_df['Price']\n",
    "    \n",
    "    x.head()\n",
    "    \n",
    "    y[:5]\n",
    "    \n",
    "    def check_collinearity(df,treshold = 0.6):\n",
    "        pairs = pd.DataFrame(columns=['feature1', 'feature2', 'value'])\n",
    "        corr_matrix = df.corr(numeric_only=True)  # Compute the correlation matrix\n",
    "        np.fill_diagonal(corr_matrix.values, 0) # Set diagonal to 0\n",
    "        #corr_matrix\n",
    "        #corr_matrix.values\n",
    "        #Create a list of correlated pairs above the threshold\n",
    "        corr = [(corr_matrix.index[x], corr_matrix.columns[y], corr_matrix.iloc[x, y]) for x, y in zip(*np.where(abs(np.tril(corr_matrix)) > treshold))]\n",
    "        # DataFrame of pairs with high correlation\n",
    "        if corr:\n",
    "            #print(corr)\n",
    "            pairs = pd.DataFrame(corr, columns=['feature1', 'feature2', 'value'])\n",
    "        return pairs\n",
    "    check_collinearity(X)\n",
    "    \n",
    "    ### Q1_3 (Advanced): From each pair of features that exhibit high collinearity, eliminate any one. Write a function to do this task (the function should take the dataframe, target and collinearity threshold as arguments and return dataframe with reduced features. Take threshold as 0.6 for this case study.\n",
    "    \n",
    "    def remove_collinearity(df,target,treshold = 0.6):\n",
    "        columns_drop = []\n",
    "        pairs = pd.DataFrame(columns=['feature1', 'feature2', 'value'])\n",
    "        corr_matrix = df.corr(numeric_only=True)  # Compute the correlation matrix\n",
    "        np.fill_diagonal(corr_matrix.values, 0) # Set diagonal to 0\n",
    "        #corr_matrix\n",
    "        #corr_matrix.values\n",
    "        #Create a list of correlated pairs above the threshold\n",
    "        corr = [(corr_matrix.index[x], corr_matrix.columns[y], corr_matrix.iloc[x, y]) for x, y in zip(*np.where(abs(np.tril(corr_matrix)) > treshold))]\n",
    "        # DataFrame of pairs with high correlation\n",
    "        if corr:\n",
    "            #print(corr)\n",
    "            pairs = pd.DataFrame(corr, columns=['feature1', 'feature2', 'value'])\n",
    "            columns_drop.append(pairs.feature1.values) \n",
    "            #print(columns_drop[0])\n",
    "            X.drop(columns=columns_drop[0] , inplace=True)\n",
    "        return x\n",
    "        \n",
    "    remove_collinearity(X,y)\n",
    "    \n",
    "    X.shape\n",
    "    \n",
    "    # Topic 2: Multi-Collinearity\n",
    "    ### Q2_1 (Basic): \n",
    "    \n",
    "    #### a) What is multicollinearity? What impact does presence of multicollinearity have on a model ?\n",
    "    \n",
    "    ##### R: Multicollinearity is the associations between predictor variables. R-squared.p-values and Coefficients are affected by multicollinearity. So if you don’t want to interpret, you don’t need to worry about Multicollinearity.\n",
    "    \n",
    "    #### b) What impact does presence of multicollinearity have on a model ?\n",
    "    \n",
    "    ##### R-squared.p-values and Coefficients are affected by multicollinearity. So if you don’t want to interpret , you don’t need to worry about Multicollinearity.\n",
    "    \n",
    "    #### c) Name a metric used to detect multicollinearity. At what values of this metric multicollinearity is present?\n",
    "    \n",
    "    ##### R: The VIF (Variance Inflation Factor) is a common metric used to detect multicollinearity in a regression model\n",
    "    \n",
    "    ### Q2_2 (Moderate): Calculate the VIF scores for all numeric features remaining in the cars dataset. How many variables have VIF scores greater than 10 \n",
    "    \n",
    "    X.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Check for the VIF values of the feature variables. \n",
    "    def check_vif_count(df,min_score=0):\n",
    "        vif = pd.DataFrame()\n",
    "        \n",
    "        vif['Features'] = df.select_dtypes(include=[np.number]).columns\n",
    "        vif['VIF'] = [variance_inflation_factor(df.select_dtypes(include=[np.number]).values, i) for i in range(df.select_dtypes(include=[np.number]).shape[1])]\n",
    "        vif['VIF'] = round(vif['VIF'], 2)\n",
    "        vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "        vif_values = vif[vif['VIF']>min_score]\n",
    "        print(vif_values)\n",
    "        print(\"-\"*30)\n",
    "        return print(f\"Quantity of variables have VIF greater than {min_score}. Total: {vif_values.count().iloc[0]} variables\")\n",
    "    \n",
    "    check_vif_count(X,min_score=10)\n",
    "    \n",
    "    ### Q2_3 (Advanced): Starting with the variable with highest VIF score, eliminate variables with VIF score greater than 10 (After eliminating a variable, recalculate the VIF scores and stop when all VIF score are below 10).\n",
    "    \n",
    "    #Create a copy of x dataset\n",
    "    X1 = x.copy()\n",
    "    \n",
    "    def check_max_vif(df):\n",
    "        vif = pd.DataFrame()\n",
    "        vif['Features'] = df.select_dtypes(include=[np.number]).columns\n",
    "        vif['VIF'] = [variance_inflation_factor(df.select_dtypes(include=[np.number]).values, i) for i in range(df.select_dtypes(include=[np.number]).shape[1])]\n",
    "        vif['VIF'] = round(vif['VIF'], 2)\n",
    "        vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "        #print(vif.to_string())\n",
    "        max_vif_value = vif.head(1)\n",
    "        return max_vif_value\n",
    "    \n",
    "    check_max_vif(X1)\n",
    "    \n",
    "    topval = 100\n",
    "    while topval > 10:\n",
    "        top = check_max_vif(X1)\n",
    "        topval = top[\"VIF\"].item()\n",
    "        if topval <=10:\n",
    "            break\n",
    "            \n",
    "        print(top)\n",
    "        X1 = X1.drop([top[\"Features\"].item()], axis =1)\n",
    "        \n",
    "    \n",
    "    #print dataframe x1 updated. \n",
    "    X1\n",
    "    \n",
    "    # Topic 3: Multiple Linear Regression\n",
    "    ### Q3_1 (Basic): \n",
    "    \n",
    "    #### a) Create a features dataset that has the remaining numeric features and relevant categorical features. Inspect the categorical features and select the relevant ones. Call this dataframes 'predictors'\n",
    "    \n",
    "    X1.info()\n",
    "    \n",
    "    #x1.select_dtypes(include=['object'])\n",
    "    print(X1['Model'].value_counts()) \n",
    "    print(X1['Fuel_Type'].value_counts())\n",
    "    print(X1['Color'].value_counts())\n",
    "    \n",
    "    # contingency_table = pd.crosstab(cars_df['Fuel_Type'], cars_df['Price'])\n",
    "    # chi2, p, _, _ = stats.chi2_contingency(contingency_table)\n",
    "    # print(f\"P-value: {p}\")\n",
    "    \n",
    "    #Deleting all categorics variables because isn't relevant \n",
    "    predictors = X1.drop(columns=['Model','Color'])\n",
    "    predictors\n",
    "    \n",
    "    #### b) Do one-hot encoding for the categorical variables in 'predictors'\n",
    "    \n",
    "    predictors = pd.get_dummies(predictors,columns=['Fuel_Type'],drop_first=True)\n",
    "    \n",
    "    predictors\n",
    "    \n",
    "    #### c) Split 'predictors' into training and test datasets.\n",
    "    \n",
    "    X_train,X_test,y_train,y_test = train_test_split(predictors,y, random_state=1,test_size=0.2)\n",
    "    \n",
    "    # Reset index to align indices\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    \n",
    "    X_train.shape,X_test.shape,y_train.shape,y_test.shape\n",
    "    \n",
    "    ### Q3_1 (Moderate):\n",
    "    \n",
    "    #### d) Standardise the training dataset.\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)  # Fit & transform training set\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train_scaled\n",
    "    \n",
    "    #### e) Use the same scaler to standardise the test dataset\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)  # Only transform test set\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    X_test_scaled\n",
    "    \n",
    "    #### f) Using statsmodel library, train an OLS model with the training dataset. Print the model summary and inspect key model metrics. What are your key observations?\n",
    "    \n",
    "    X_train_scaled = sm.add_constant(X_train_scaled)\n",
    "    ols = sm.OLS(y_train,X_train_scaled)\n",
    "    model = ols.fit()\n",
    "    print(model.summary())\n",
    "    \n",
    "     -  ##### Prob (F-statistic): Since the P-value is less 0.05% (significance level) we REJECT THE NULL HYPOTHESIS. We conclude \"there IS sufficient evidence to conclude that at least one independent variable (predictor) has a significant relationship with the dependent variable (Price).\n",
    "     -  ##### R-Square: 0.730 means that 73.0% of the variation in car prices can be explained by the independent variables.\n",
    "     -  ##### P>|t|: Since the p-value of independent variable is less 5% we REJECT the nul hypothesis and conclude that this variables has a correlations with dependet variable. The variables: \"Met_Color, Automatic, Sport_Model, Metallic_Rim, and Parking_Assistant do not significantly influence the price of a car at a 5% significance level.\"\n",
    "    \n",
    "    ### Q3_3 (Advanced): \n",
    "    \n",
    "    #### g) Using the model, make predictions of price for the test set.\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "    \n",
    "    y_test\n",
    "    \n",
    "    y_pred_sklearn = lr.predict(X_test)\n",
    "    y_pred_sklearn[:5]\n",
    "    \n",
    "    #### h) Compute the model score (r2, adjusted r2) for the test set.\n",
    "    \n",
    "    lr.fit(X_train,y_train)\n",
    "    y_pred_test = linear_regressor.predict(X_test)\n",
    "    r2_val_test = r2_score(y_test,y_pred_test)\n",
    "    r2_adj_val_test=1 - (((len(X_test.index) - 1) / (len(X_test.index) - len(X_test.columns) - 1)) * (1 - r2_score(y_test,y_pred_test)))\n",
    "    \n",
    "    rmse_error_test = root_mean_squared_error (y_test, y_pred_test)\n",
    "    print (\"R2 score (test) for the model is :\",r2_val_test )\n",
    "    print(\"Adjusted_R2(test) for the model is :\",r2_adj_val_test)\n",
    "    print (\"RMSE error (test) for the model is :\",rmse_error_test )\n",
    "    \n",
    "    #### i) Compare the test scores with the training scores. Do you observe any signs of overfitting. Why / why not ?\n",
    "    \n",
    "    y_pred_train = lr.predict(X_train)\n",
    "    r2_val_train = r2_score(y_train,y_pred_train)\n",
    "    print (\"R2 score (test) for the model is :\",r2_val_train )\n",
    "    \n",
    "    # Check for overfitting\n",
    "    if r2_val_train > r2_val_test + 0.1:\n",
    "        print(\"Possible Overfitting: Model performs much better on training data than test data.\")\n",
    "    elif r2_val_train < r2_val_test:\n",
    "        print(\"Possible Underfitting: Model underperforms on training data.\")\n",
    "    else:\n",
    "        print(\"No significant overfitting: Model generalizes well.\")\n",
    "    \n",
    "    # Topic 4: Feature Selection\n",
    "    ### Q4_1 (Basic): \n",
    "    \n",
    "    #### a) Use any feature selection method to eliminate features that are not adding any value to the model.\n",
    "    \n",
    "    #backward feature elimination\n",
    "    maxp = model.pvalues.max()\n",
    "    columns_drop = []\n",
    "    while(maxp > 0.05):\n",
    "        X_train_scaled.drop(model.pvalues.idxmax(),axis=1,inplace=True) \n",
    "        print(f\"Adjuste R-Square is {model.rsquared_adj}\")\n",
    "        print(f\"{model.pvalues.idxmax()} with p-value= {maxp} was dropped\\n\")\n",
    "        ols = sm.OLS(y_train,X_train_scaled)\n",
    "        model = ols.fit()\n",
    "        maxp = model.pvalues.max()\n",
    "        columns_drop.append(model.pvalues.idxmax())\n",
    "    print(model.summary())\n",
    "    \n",
    "    #### b) How many faetures were eliminated? Were all features having p-value > 0.05 originally eliminated ?\n",
    "    \n",
    "    print(columns_drop)\n",
    "    \n",
    "    Adjuste R-Square is 0.7250561725057721\n",
    "    Parking_Assistant with p-value= 0.371404498988378 was dropped\n",
    "    \n",
    "    Adjuste R-Square is 0.7251050193903597\n",
    "    Automatic with p-value= 0.433405556128329 was dropped\n",
    "    \n",
    "    Adjuste R-Square is 0.7251989757079091\n",
    "    Met_Color with p-value= 0.2995007174511488 was dropped\n",
    "    \n",
    "    Adjuste R-Square is 0.7251801514749718\n",
    "    Sport_Model with p-value= 0.09985598749764288 was dropped\n",
    "    \n",
    "    Adjuste R-Square is 0.7247640848864247\n",
    "    Metallic_Rim with p-value= 0.05690389274843182 was dropped\n",
    "    \n",
    "    #### c)What do you observe in the performance metrics of the model with reduced features?\n",
    "    \n",
    "    All this features eliminated, just represent 0.02% of price of cars, then when we drop theses variables we make our model less complex. \n",
    "    \n",
    "    ### Q4_2 (Moderate + Advanced): \n",
    "    \n",
    "    #### d) Update the test dataset by removing the variables that have been eliminated.\n",
    "    \n",
    "    X_test_scaled.columns\n",
    "    \n",
    "    X_test_scaled.drop(columns=columns_drop, inplace=True)\n",
    "    X_test_scaled.columns\n",
    "    X_test_scaled = X_test_scaled.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    #### e) Using the model, make predictions of price for the test set.\n",
    "    \n",
    "    X_test_scaled = sm.add_constant(X_test_scaled)\n",
    "    ols = sm.OLS(y_test,X_test_scaled)\n",
    "    model = ols.fit()\n",
    "    print(model.summary())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### f) Compute the model score (r2, adjusted r2) for the test set.\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_test_scaled,y_test)\n",
    "    \n",
    "    lr.fit(X_test,y_test)\n",
    "    y_pred_test = linear_regressor.predict(X_test)\n",
    "    r2_val_test = r2_score(y_test,y_pred_test)\n",
    "    r2_adj_val_test=1 - (((len(X_test.index) - 1) / (len(X_test.index) - len(X_test.columns) - 1)) * (1 - r2_score(y_test,y_pred_test)))\n",
    "    \n",
    "    rmse_error_test = root_mean_squared_error (y_test, y_pred_test)\n",
    "    print (\"R2 score (test) for the model is :\",r2_val_test )\n",
    "    print(\"Adjusted_R2(test) for the model is :\",r2_adj_val_test)\n",
    "    print (\"RMSE error (test) for the model is :\",rmse_error_test )\n",
    "    \n",
    "    #### g) Compare the test scores with the training scores. Do you observe any signs of overfitting. Why / why not ?\n",
    "    \n",
    "    # Topic 5: Cross-Validation\n",
    "    ### Q5_1 (Basic):\n",
    "    \n",
    "    #### a) Apply 5-fold cross validationto the training set.\n",
    "    \n",
    "    # Cross validation scores for the model. \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    crossvalidation = cross_val_score(linear_regressor, X_train_scaled, y_train, cv=5, n_jobs=-1)\n",
    "    crossvalidation\n",
    "    \n",
    "    #### b) What is the average r2 score ? what does this tell you ?\n",
    "    \n",
    "    avg_r2 = np.mean(crossvalidation)\n",
    "    print(f\"Average r2 Score: {avg_r2:.4f}\")\n",
    "    print(\"The model explains 70.7% of the variance in the target variable across different validation splits.\")\n",
    "    \n",
    "    ### Q5_2 (Moderate):\n",
    "    \n",
    "    #### a) Use \"explained variance\" as the scoring method and apply 5-fold cross validation, while applying shuffle to the fold creation. \n",
    "    \n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    cv = KFold(n_splits=5, random_state=1, shuffle=True) \n",
    "    \n",
    "    #build multiple linear regression model\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    \n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, scoring='explained_variance',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    \n",
    "    scores\n",
    "    \n",
    "    #### b) What do you observe ?\n",
    "    \n",
    "    When we use shuffle, and this method our fold models incresead the a lit bit the confiability of model. \n",
    "    \n",
    "    ### Q5_3 (Advanced):\n",
    "    \n",
    "    #### a) Use \"mean squared error\" as the scoring method and apply 5-fold cross validation, while applying shuffle to the fold creation. \n",
    "    \n",
    "    cv = KFold(n_splits=5, random_state=1, shuffle=True) \n",
    "    \n",
    "    #build multiple linear regression model\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    \n",
    "    means = cross_val_score(model, X_train_scaled, y_train, scoring='neg_mean_squared_error',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    \n",
    "    np.sqrt(np.mean(np.absolute(means)))\n",
    "    \n",
    "    #### b) What does the result say about the model?\n",
    "    \n",
    "    From the output we can see that the root mean squared error (RMSE) was 1895.22. The high the RMSE, the more far a model is able to predict the actual observations.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab2b6e-45a6-4a2b-ab18-1d43af24510f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
